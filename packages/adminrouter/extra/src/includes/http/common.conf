# Enable VTS module for metrics.
#
# The VTS module uses shared memory to store metrics data.
# In particular, we store metrics data generated by the
# ``vhost_traffic_status_filter_by_set_key`` directive.
#
# The amount of memory allocated here is allocated on each DC/OS node.
# The number of metrics data items (called nodes) stored on each DC/OS node
# scales with the number of DC/OS nodes in the cluster.
#
# Allocating more memory (and therefore more metrics nodes):
# - (+) If we hit the maximum node count, we will remove old data - this only
#       is an issue if this data has not been scraped, so the more memory we
#       have, the more flexible we can be with the scraping interval.
# - (-) Increased memory usage and requirements for DC/OS.
# - (-) The more nodes there are, the slower the scrape takes, so the larger
#       the scraping interval has to be.
#
# The shared memory zone size must be sufficiently large to ensure that all
# metrics that could potentially be generated by the VTS module's
# vhost_traffic_status_filter_by_set_key directive can be stored. If the
# shared memory zone limit is hit Admin Router will not create any new metric
# nodes and it will print error messages such as `ngx_slab_alloc() failed: no
# memory in vhost_traffic_status_zone "ngx_http_vhost_traffic_status"`.
#
# See https://jira.mesosphere.com/browse/DCOS_OSS-4695 for an issue to create
# a test which shows that Admin Router survives when the shared memory is full.
#
# Set shared memory size to 16MB. This accommodates ~4000 metric nodes. Reserve
# 1300 metric nodes for Admin Router agent forwarding where each agent is
# counted as distinct upstream in the order of at maximum 1000 agents.
# Upstreams are seemingly not accounted for by the VTS module LRU mechanism so
# we must always keep enough room for them.

vhost_traffic_status_zone shared:vhost_traffic_status:16m;

# The metrics data scales with the number of metric nodes. A metric node in our
# case is a set of data about one detail, such as the number of requests made
# with a particular client, or the number of responses with a particular status
# code, or the number of URIs requested from Admin Router. In theory, the
# number of URIs can be infinitely large. Therefore, we risk running out of
# memory if the number of URIs requested grows large, unless we limit the
# number of metrics nodes with the LRU mechanism provided in the VTS module.
#
# Set max allowed metric nodes to 2700 at which the LRU mechanism kicks in.
# Caution: The LRU mechanism seemingly only accounts for metric nodes created
# by the ``vhost_traffic_status_filter_by_set_key`` directive.

vhost_traffic_status_filter_max_node 2700;

# This implies in the worst-case metrics for 2700 / 3 (for status, URI, client)
# = 900 requests can be safely stored at any given time. Given a Telegraf
# scrape interval of 10 seconds this means 90 requests/s is a safe rate at
# which we expect no metrics to be missed despite the worst-case (status, URI,
# client) tuple distribution. In MWT #11 we did not see 100 requests/s being
# reached across all Master Admin Router instances so this should suffice.
#
# The aforementioned numbers were evaluated in the following experiment:
#
# 1. Call random 128 characters long random URLs on Admin Router in a loop.
#
# 2. Wait for the shared memory to fill up until the LRU mechanism activates.
# This resulted in in 2701 nodes being used -> 9.2MB additional metric data.
#
# 3. Node internal measurements (Telegraf perspective):
# - curl http://localhost/nginx/metric (Downloading 10.6MB of data takes < 1
# second, nearly instant even.)
#
# 4. Node external measurements (Telegraf endpoint):
# - curl http://172.17.0.2:61091/metrics takes < 3 seconds on Docker
# (Downloading 53MB of metric data on an otherwise empty cluster)
#
# The scraping interval of Prometheus in the dcos-monitoring package currently
# is 60 seconds and the timeout at 50 seconds. This means it would need to
# scrape the Telegraf endpoint at roughly 1MB/s when the shared memory is
# filled up in this experiment.
#
# Going forward we will implement a dedicated Admin Router Telegraf plugin
# which vastly reduces the number of reported useless metrics.
# From then on, only the node internal scraping time for Nginx metrics is
# impacted by the settings above, which eases decision making on the
# maintainers of this configuration. Until then, this the settings above are
# believed to present a reasonable trade-off to accommodate metrics for
# internal testing and not prevent Prometheus from falling behind on scraping
# metrics from Telegraf running on each DC/OS node.

client_max_body_size 1024M;

# Define custom log format.
log_format customformat '$time_local $pid *$connection '
    '$remote_addr:$remote_port> $host :$server_port $ssl_protocol $ssl_cipher '
    '$request $http_user_agent $http_referer len=$request_length '
    '<$upstream_addr code=$status len=$bytes_sent($body_bytes_sent) '
    '$upstream_connect_time $upstream_header_time-$upstream_response_time $request_time';

# The syslog facility here is set to daemon because
# systemd SyslogFacility defaults to daemon and
# therefore all other DC/OS services log to it.
# https://jira.mesosphere.com/browse/DCOS-38622
access_log syslog:server=unix:/dev/log,facility=daemon customformat;
include mime.types;
default_type application/octet-stream;
sendfile on;
keepalive_timeout 65;

server_tokens off;

lua_package_path '$prefix/conf/lib/?.lua;;';


# Name: DC/OS Component Package Manager (Pkgpanda)
# Reference: https://docs.mesosphere.com/1.12/administering-clusters/component-management/
upstream pkgpanda {
    server unix:/run/dcos/pkgpanda-api.sock;
}

# Name: DC/OS Log
# Reference: https://docs.mesosphere.com/1.12/monitoring/logging/logging-api/
upstream log {
    server unix:/run/dcos/dcos-log.sock;
}

# Name: DC/OS Checks API
upstream dcos_checks_api {
    server unix:/run/dcos/dcos-checks-api.sock;
}
